diff --git a/README.md b/README.md
index bf2dcf0..8d65023 100644
--- a/README.md
+++ b/README.md
@@ -29,4 +29,9 @@ loss = loss_fn(output, output)  # loss = tensor(38.8402, grad_fn=<AddBackward0>)
 
 ## Results
 
-Will be added shortly.
\ No newline at end of file
+| Loss          | 32 Batch Size | 64 Batch Size |
+| ------------- | ------------- | ------------- |
+| Cross Entropy | 78.3          | 81.47         |
+| DCL           | 84.6          | 85.57         |
+| DCLW          | 83.32         | 82.68         |
+
diff --git a/experiment/__init__.py b/experiment/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/experiment/simclr/linear.py b/experiment/simclr/linear.py
new file mode 100644
index 0000000..705c208
--- /dev/null
+++ b/experiment/simclr/linear.py
@@ -0,0 +1,102 @@
+import argparse
+
+import os
+import pandas as pd
+import torch
+import torch.nn as nn
+import torch.optim as optim
+from torch.utils.data import DataLoader
+from torchvision.datasets import CIFAR10
+from tqdm import tqdm
+
+from experiment.simclr import utils
+from experiment.simclr.model import Model
+
+
+class Net(nn.Module):
+    def __init__(self, num_class, pretrained_path):
+        super(Net, self).__init__()
+
+        # encoder
+        self.f = Model().f
+        # classifier
+        self.fc = nn.Linear(2048, num_class, bias=True)
+        self.load_state_dict(torch.load(pretrained_path, map_location='cpu'), strict=False)
+
+    def forward(self, x):
+        x = self.f(x)
+        feature = torch.flatten(x, start_dim=1)
+        out = self.fc(feature)
+        return out
+
+
+# train or test for one epoch
+def train_val(net, data_loader, train_optimizer, loss_criterion, epoch, epochs):
+    is_train = train_optimizer is not None
+    net.train() if is_train else net.eval()
+
+    total_loss, total_correct_1, total_correct_5, total_num, data_bar = 0.0, 0.0, 0.0, 0, tqdm(data_loader)
+    with (torch.enable_grad() if is_train else torch.no_grad()):
+        for data, target in data_bar:
+            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)
+            out = net(data)
+            loss = loss_criterion(out, target)
+
+            if is_train:
+                train_optimizer.zero_grad()
+                loss.backward()
+                train_optimizer.step()
+
+            total_num += data.size(0)
+            total_loss += loss.item() * data.size(0)
+            prediction = torch.argsort(out, dim=-1, descending=True)
+            total_correct_1 += torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()
+            total_correct_5 += torch.sum((prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()
+
+            data_bar.set_description('{} Epoch: [{}/{}] Loss: {:.4f} ACC@1: {:.2f}% ACC@5: {:.2f}%'
+                                     .format('Train' if is_train else 'Test', epoch, epochs, total_loss / total_num,
+                                             total_correct_1 / total_num * 100, total_correct_5 / total_num * 100))
+
+    return total_loss / total_num, total_correct_1 / total_num * 100, total_correct_5 / total_num * 100
+
+
+def run_test():
+    parser = argparse.ArgumentParser(description='Linear Evaluation')
+    parser.add_argument('--model_path', type=str, default='results/128_0.5_200_512_500_model.pth',
+                        help='The pretrained model path')
+    parser.add_argument('--batch_size', type=int, default=512, help='Number of images in each mini-batch')
+    parser.add_argument('--epochs', type=int, default=100, help='Number of sweeps over the dataset to train')
+
+    args = parser.parse_args()
+    model_path, batch_size, epochs = args.model_path, args.batch_size, args.epochs
+    train_data = CIFAR10(root='data', train=True, transform=utils.train_transform, download=True)
+    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)
+    test_data = CIFAR10(root='data', train=False, transform=utils.test_transform, download=True)
+    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)
+
+    model = Net(num_class=len(train_data.classes), pretrained_path=model_path).cuda()
+    for param in model.f.parameters():
+        param.requires_grad = False
+
+    optimizer = optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6)
+    loss_criterion = nn.CrossEntropyLoss()
+    results = {'train_loss': [], 'train_acc@1': [], 'train_acc@5': [],
+               'test_loss': [], 'test_acc@1': [], 'test_acc@5': []}
+
+    best_acc = 0.0
+    for epoch in range(1, epochs + 1):
+        train_loss, train_acc_1, train_acc_5 = train_val(model, train_loader, optimizer, loss_criterion, epoch, epochs)
+        results['train_loss'].append(train_loss)
+        results['train_acc@1'].append(train_acc_1)
+        results['train_acc@5'].append(train_acc_5)
+        test_loss, test_acc_1, test_acc_5 = train_val(model, test_loader, None, loss_criterion, epoch, epochs)
+        results['test_loss'].append(test_loss)
+        results['test_acc@1'].append(test_acc_1)
+        results['test_acc@5'].append(test_acc_5)
+        # save statistics
+        data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))
+        output_name = os.splitext(os.split(args.model_path)[1])[0]
+        data_frame.to_csv(f'results/{output_name}_linear_statistics.csv', index_label='epoch')
+        if test_acc_1 > best_acc:
+            best_acc = test_acc_1
+            torch.save(model.state_dict(), f'results/{output_name}_linear_model.pth')
diff --git a/experiment/simclr/main.py b/experiment/simclr/main.py
new file mode 100644
index 0000000..e1fc1af
--- /dev/null
+++ b/experiment/simclr/main.py
@@ -0,0 +1,147 @@
+import argparse
+import os
+
+import pandas as pd
+import torch
+import torch.optim as optim
+from torch.utils.data import DataLoader
+from tqdm import tqdm
+
+from experiment.simclr import utils
+from experiment.simclr.model import Model
+
+
+# train for one epoch to learn unique features
+from loss import DCL
+from loss.dcl import DCLW
+
+
+def train(net, data_loader, train_optimizer, args, epoch):
+    net.train()
+    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)
+    for pos_1, pos_2, target in train_bar:
+        pos_1, pos_2 = pos_1.cuda(non_blocking=True), pos_2.cuda(non_blocking=True)
+        feature_1, out_1 = net(pos_1)
+        feature_2, out_2 = net(pos_2)
+        if args.loss == 'dcl':
+            l = DCL(temperature=args.temperature)
+            loss = l(out_1, out_2)
+        elif args.loss == 'dclw':
+            l = DCLW(temperature=args.temperature)
+            loss = l(out_1, out_2)
+        elif args.loss == 'ce':
+            # [2*B, D]
+            out = torch.cat([out_1, out_2], dim=0)
+            # [2*B, 2*B]
+            sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / args.temperature)
+            mask = (torch.ones_like(sim_matrix) - torch.eye(2 * args.batch_size, device=sim_matrix.device)).bool()
+            # [2*B, 2*B-1]
+            sim_matrix = sim_matrix.masked_select(mask).view(2 * args.batch_size, -1)
+
+            # compute loss
+            pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / args.temperature)
+            # [2*B]
+            pos_sim = torch.cat([pos_sim, pos_sim], dim=0)
+            loss = (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean()
+
+        train_optimizer.zero_grad()
+        loss.backward()
+        train_optimizer.step()
+
+        total_num += args.batch_size
+        total_loss += loss.item() * args.batch_size
+        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f}'.format(epoch, args.epochs, total_loss / total_num))
+
+    return total_loss / total_num
+
+
+# test for one epoch, use weighted knn to find the most similar images' label to assign the test image
+def test(net, memory_data_loader, test_data_loader, args, epoch):
+    net.eval()
+    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []
+    with torch.no_grad():
+        # generate feature bank
+        for data, _, target in tqdm(memory_data_loader, desc='Feature extracting'):
+            feature, out = net(data.cuda(non_blocking=True))
+            feature_bank.append(feature)
+        # [D, N]
+        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()
+        # [N]
+        feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)
+        # loop test data to predict the label by weighted knn search
+        test_bar = tqdm(test_data_loader)
+        for data, _, target in test_bar:
+            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)
+            feature, out = net(data)
+
+            total_num += data.size(0)
+            # compute cos similarity between each feature vector and feature bank ---> [B, N]
+            sim_matrix = torch.mm(feature, feature_bank)
+            # [B, K]
+            sim_weight, sim_indices = sim_matrix.topk(k=args.k, dim=-1)
+            # [B, K]
+            sim_labels = torch.gather(feature_labels.expand(data.size(0), -1), dim=-1, index=sim_indices)
+            sim_weight = (sim_weight / args.temperature).exp()
+
+            # counts for each class
+            one_hot_label = torch.zeros(data.size(0) * args.k, args.c, device=sim_labels.device)
+            # [B*K, C]
+            one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)
+            # weighted score ---> [B, C]
+            pred_scores = torch.sum(one_hot_label.view(data.size(0), -1, args.c) * sim_weight.unsqueeze(dim=-1), dim=1)
+
+            pred_labels = pred_scores.argsort(dim=-1, descending=True)
+            total_top1 += torch.sum((pred_labels[:, :1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()
+            total_top5 += torch.sum((pred_labels[:, :5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()
+            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}% Acc@5:{:.2f}%'
+                                     .format(epoch, args.epochs, total_top1 / total_num * 100, total_top5 / total_num * 100))
+
+    return total_top1 / total_num * 100, total_top5 / total_num * 100
+
+
+def run_train():
+    parser = argparse.ArgumentParser(description='Train SimCLR')
+    parser.add_argument('--feature_dim', default=128, type=int, help='Feature dim for latent vector')
+    parser.add_argument('--temperature', default=0.5, type=float, help='Temperature used in softmax')
+    parser.add_argument('--k', default=200, type=int, help='Top k most similar images used to predict the label')
+    parser.add_argument('--batch_size', default=512, type=int, help='Number of images in each mini-batch')
+    parser.add_argument('--epochs', default=500, type=int, help='Number of sweeps over the dataset to train')
+    parser.add_argument('--loss', default='ce', type=str, help='loss function')
+
+    # args parse
+    args = parser.parse_args()
+    feature_dim, temperature, k = args.feature_dim, args.temperature, args.k
+    batch_size, epochs = args.batch_size, args.epochs
+
+    # data prepare
+    train_data = utils.CIFAR10Pair(root='data', train=True, transform=utils.train_transform, download=True)
+    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True,
+                              drop_last=True)
+    memory_data = utils.CIFAR10Pair(root='data', train=True, transform=utils.test_transform, download=True)
+    memory_loader = DataLoader(memory_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)
+    test_data = utils.CIFAR10Pair(root='data', train=False, transform=utils.test_transform, download=True)
+    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)
+
+    # model setup and optimizer config
+    model = Model(feature_dim).cuda()
+    optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)
+    args.c = len(memory_data.classes)
+
+    # training loop
+    results = {'train_loss': [], 'test_acc@1': [], 'test_acc@5': []}
+    save_name_pre = '{}_{}_{}_{}_{}'.format(feature_dim, temperature, k, batch_size, epochs)
+    if not os.path.exists('results'):
+        os.mkdir('results')
+    best_acc = 0.0
+    for epoch in range(1, epochs + 1):
+        train_loss = train(model, train_loader, optimizer, args, epoch)
+        results['train_loss'].append(train_loss)
+        test_acc_1, test_acc_5 = test(model, memory_loader, test_loader, args, epoch)
+        results['test_acc@1'].append(test_acc_1)
+        results['test_acc@5'].append(test_acc_5)
+        # save statistics
+        data_frame = pd.DataFrame(data=results, index=range(1, epoch + 1))
+        data_frame.to_csv('results/{}_statistics.csv'.format(save_name_pre), index_label='epoch')
+        if test_acc_1 > best_acc:
+            best_acc = test_acc_1
+            torch.save(model.state_dict(), 'results/{}_model.pth'.format(save_name_pre))
diff --git a/experiment/simclr/model.py b/experiment/simclr/model.py
new file mode 100644
index 0000000..2406acc
--- /dev/null
+++ b/experiment/simclr/model.py
@@ -0,0 +1,27 @@
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torchvision.models.resnet import resnet50
+
+
+class Model(nn.Module):
+    def __init__(self, feature_dim=128):
+        super(Model, self).__init__()
+
+        self.f = []
+        for name, module in resnet50().named_children():
+            if name == 'conv1':
+                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
+            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):
+                self.f.append(module)
+        # encoder
+        self.f = nn.Sequential(*self.f)
+        # projection head
+        self.g = nn.Sequential(nn.Linear(2048, 512, bias=False), nn.BatchNorm1d(512),
+                               nn.ReLU(inplace=True), nn.Linear(512, feature_dim, bias=True))
+
+    def forward(self, x):
+        x = self.f(x)
+        feature = torch.flatten(x, start_dim=1)
+        out = self.g(feature)
+        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)
diff --git a/experiment/simclr/utils.py b/experiment/simclr/utils.py
new file mode 100644
index 0000000..a216d5b
--- /dev/null
+++ b/experiment/simclr/utils.py
@@ -0,0 +1,34 @@
+from PIL import Image
+from torchvision import transforms
+from torchvision.datasets import CIFAR10
+
+
+class CIFAR10Pair(CIFAR10):
+    """CIFAR10 Dataset.
+    """
+
+    def __getitem__(self, index):
+        img, target = self.data[index], self.targets[index]
+        img = Image.fromarray(img)
+
+        if self.transform is not None:
+            pos_1 = self.transform(img)
+            pos_2 = self.transform(img)
+
+        if self.target_transform is not None:
+            target = self.target_transform(target)
+
+        return pos_1, pos_2, target
+
+
+train_transform = transforms.Compose([
+    transforms.RandomResizedCrop(32),
+    transforms.RandomHorizontalFlip(p=0.5),
+    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),
+    transforms.RandomGrayscale(p=0.2),
+    transforms.ToTensor(),
+    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])
+
+test_transform = transforms.Compose([
+    transforms.ToTensor(),
+    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])
diff --git a/loss/dcl.py b/loss/dcl.py
index 5affef9..671e236 100644
--- a/loss/dcl.py
+++ b/loss/dcl.py
@@ -42,5 +42,5 @@ class DCLW(DCL):
     temperature: temperature to control the sharpness of the distribution
     """
     def __init__(self, sigma=0.5, temperature=0.1):
-        weight_fn = lambda z1, z2: 2 - z1.size(0) * torch.nn.functional.softmax((z1 * z2).sum(dim=1) / sigma, dim=0).squeeze()
+        weight_fn = lambda z1, z2: 2 - torch.nn.functional.softmax((z1 * z2).sum(dim=1) / sigma, dim=0).squeeze()
         super(DCLW, self).__init__(weight_fn=weight_fn, temperature=temperature)
diff --git a/run_simclr.sh b/run_simclr.sh
new file mode 100644
index 0000000..e1ebf01
--- /dev/null
+++ b/run_simclr.sh
@@ -0,0 +1,25 @@
+#!/bin/bash
+#SBATCH --job-name=simclr
+#SBATCH --gres=gpu:1
+#SBATCH --cpus-per-task=8
+#SBATCH --mem=60GB
+#SBATCH -o %j.out
+#SBATCH -e %j.out
+
+DATASET=CIFAR10
+BATCH_SIZE=128
+LOSS="dclw"
+TEMP=0.1
+
+python train.py \
+  --batch_size $BATCH_SIZE \
+  --epochs 100 \
+  --feature_dim 128 \
+  --loss $LOSS \
+  --temperature $TEMP \
+  --model_path $PATH
+
+python test.py \
+  --batch_size 64 \
+  --epochs 100 \
+  --model_path "results/128_${TEMP}_200_${BATCH_SIZE}_100_${LOSS}_model.pth"
\ No newline at end of file
diff --git a/scripts/run_simclr.sh b/scripts/run_simclr.sh
new file mode 100644
index 0000000..1db11ae
--- /dev/null
+++ b/scripts/run_simclr.sh
@@ -0,0 +1,24 @@
+#!/bin/bash
+#SBATCH --job-name=simclr
+#SBATCH --gres=gpu:1
+#SBATCH --cpus-per-task=8
+#SBATCH --mem=60GB
+#SBATCH -o %j.out
+#SBATCH -e %j.out
+
+PATH="save/dclw_b128_e100_res18_cifar10"
+DATASET=CIFAR10
+BATCH_SIZE=128
+LOSS="dclw"
+
+python /projects/ovcare/classification/ramin/ml/Decoupled-Contrastive-Learning/train.py \
+  --batch_size $BATCH_SIZE \
+  --epochs 100 \
+  --feature_dim 128 \
+  --loss $LOSS \
+  --model_path $PATH
+
+python /projects/ovcare/classification/ramin/ml/Decoupled-Contrastive-Learning/test.py \
+  --batch_size 512 \
+  --epochs 100 \
+  --model_path "${PATH}/128_0.5_200_${BATCH_SIZE}_100_model.path"
\ No newline at end of file
diff --git a/test.py b/test.py
new file mode 100644
index 0000000..83add12
--- /dev/null
+++ b/test.py
@@ -0,0 +1,3 @@
+from experiment.simclr.linear import run_test
+
+run_test()
\ No newline at end of file
diff --git a/train.py b/train.py
new file mode 100644
index 0000000..293bfe4
--- /dev/null
+++ b/train.py
@@ -0,0 +1,3 @@
+from experiment.simclr.main import run_train
+
+run_train()
